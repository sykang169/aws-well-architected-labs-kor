[
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/review/1_reviewagine/",
	"title": "Again AWS Well-Architected Review using Well-Architected tool",
	"tags": [],
	"description": "",
	"content": " Review based on pillars with best practices applied.\n   Log-in AWS console, typing Well-Architected Tools in service search bar. you can go to this link : https://console.aws.amazon.com/wellarchitected/.\n  In the previously created workload, Workload for AWS Workshop, click AWS Well-Architected Framework. Then click on each filler to check the question that corresponds to the best practice applied.\n  operational excellence as an example. Select Operational Excellence at the bottom. you can go to this link: https://console.aws.amazon.com/wellarchitected/.   In Operational Excellence labs, we managed and monitored system configuration and application using the inventory and State manager of Systems Manager. In addition, detailed information was organized to gain visibility into compliance status. And, we have configured the patch manager to automatically select and deploy operating system and software patches to all EC2 instances. These tasks can be very helpful in monitoring instances deployed in production, reducing defects, and automating patches and configurations.\nQuestions that apply to the task OPS 5. How do you reduce defects, ease remediation, and improve flow into production? Click.   uncheck None of these, click Use configuration management systems and Perform patch management. You can also click info in the question to find out more. There is still more to be improved. Only 2 out of 11 questions were chosen. Being able to choose more questions can reduce your risk. Scroll down to see the various enhancements we can apply.   When finished, click Save and exit at the top.\n  In this way, other pillars review again the question. To reduce the risk of the AWS Well-Architected Framework, it is necessary to apply a variety of best practices and repeat reviews. In real workloads, review periodically. It eliminates the risk of architecture and creates a scalable and secure architecture. If necessary, you can also conduct a review with a partner or SA.\n  You can get more information at https://wellarchitectedlabs.com/.\n  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/security/200_cloudfront_with_waf_protection/1_config_waf/",
	"title": "AWS WAF Config",
	"tags": [],
	"description": "",
	"content": "  Open the AWS WAF Console: https://console.aws.amazon.com/wafv2/homev2\n  From left menu, select Getting started, click Create web ACL.   first setting resource type to CloudFront distributions. and type value in each text bar like this.\n   Name : well-architected-cf-waf-acl Description : for well-architected CF WAF\nAnd click next, and click Create web ACL for finish configuration.  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/setup/wellarchitectedtools/2_create_workload/",
	"title": "Creating a workload",
	"tags": [],
	"description": "",
	"content": "  Log-in AWS console, typing Well-Architected Tools in service search bar. you can go to this link : https://console.aws.amazon.com/wellarchitected/.\nWell-Architected Reviews are conducted per workload. A workload identifies a set of components that deliver business value. The workload is usually the level of detail that business and technology leaders communicate about.\n  Click the Define Workload button on the landing page:   If you have already created a workload, a list of workloads will appear. Similarly, on this screen, click the Define Workload button.   On the Define Workload interface, enter the necessary information:\n  Name: Workload for AWS Workshop\n  Description: wellarchitectedlabs workshop\n  Environment: Select Pre-production\n  Review owner: workload owner`s email addres or alias.\n  Account ID: \u0026lt;AWS ACCOUNT_NUMBER\u0026gt;\n  Regions: Select AWS Regions, example:(Seoul)/ap-northeast-2\n  Industry Type: InfoTech(option)\n  Industry: Internet(option)\nAll finished, Click Next button:\n    in Apply lenses tab, Click checkbox AWS Well-Architected Framework. AWS Well-Architected Framework The AWS Well-Architected Tool helps you review the state of your workloads and compares them to the latest AWS architectural best practices. You can also choose a other Lense for your workload which specific workload.: Click Define workload and finished.\n  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/100_aws_resource_optimization/1_cloudwatch_intro/",
	"title": "Getting to know Amazon Cloudwatch",
	"tags": [],
	"description": "",
	"content": "The first step to perform right sizing is to monitor and analyze your current use of services to gain insight into instance performance and usage patterns. To gather sufficient data, observe performance over at least a two-week period (ideally, over a one-month period) to capture the workload and business peak. The most common metrics that define instance performance are vCPU utilization, memory utilization, network utilization, and disk use.\n  Log into your AWS console via SSO, go to the Amazon CloudWatch service page:   Select EC2 under the Service Dashboard:   Observe the Service Dashboard and all of its different metrics, but focus on CPU Utilization and Network In and Out:   Select one of the EC2 resources by clicking on the little color icon to the left of the resource-id name:   Deselect the EC2 resource and now modify the time range on the top right, click custom and select the last 2 weeks:   Navigate to the CPU Utilization Average widget, click the three dots and launch the View in metrics page. Using the Graphed metrics section try to answer the following questions:\n   a. What is the instance with the highest CPU Average? b. What is the instance with the highest CPU Max? c. What is the instance with the lowest CPU Min?  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/200_aws_resource_optimization/2_create_iamrole/",
	"title": "Getting to know Amazon Cloudwatch",
	"tags": [],
	"description": "",
	"content": "Access to AWS resources requires permissions. You will now create an IAM role to grant permissions that the agent needs to write metrics to CloudWatch. Amazon created two new default policies called CloudWatchAgentServerPolicy and CloudWatchAgentAdminPolicy only for that purpose.\n  To create the IAM role first you will need to sign in to the AWS Management Console and open the IAM console   In the navigation pane on the left, choose Roles and then Create role.   Under Choose the service that will use this role, choose EC2 Allows EC2 instances to call AWS services on your behalf. Choose Next: Permissions.   In the list of policies, select the check box next to CloudWatchAgentServerPolicy. If necessary, use the search box to find the policy, click Next: Tags:\n    Add tags (optional) for this policy, click Next: Review.   Confirm that CloudWatchAgentServerPolicy appears next to Policies. In Role name, enter a name for the role, such as CloudWatchAgentServerRole. Optionally give it a description. Then click Create role.   The role is now created.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/reliability/test_resiliency/3_failure_injection_prep/",
	"title": "Preparation for Failure Injection",
	"tags": [],
	"description": "",
	"content": "Failure injection (also known as chaos testing) is an effective and essential method to validate and understand the resiliency of your workload and is a recommended practice of the AWS Well-Architected Reliability Pillar. Here you will initiate various failure scenarios and assess how your system reacts.\nPreparation Before testing, please prepare the following:\n  Get VPC ID\n A VPC (Amazon Virtual Private Cloud) is a logically isolated section of the AWS Cloud where you have deployed the resources for your service For these tests you will need to know the VPC ID of the VPC you created as part of deploying the service Navigate to the VPC management console: https://console.aws.amazon.com/vpc In the left pane, click Your VPCs * 1 - Tick the checkbox next to WellArchitectedLabsStack/VPC * 2 - Copy the VPC ID   Save the VPC ID - you will use later whenever \u0026lt;vpc-id\u0026gt; is indicated in a command    Let`s move the script that will be used for fault injection to Cloud9. Download the file below.\n fail_az.sh fail_instance.sh failover_rds.sh    Start Cloud9(Open AWS Cloud9 Console로). Select WellArchitectedWorkshop's Cloud9 in console.   Upload download file to Cloud9. top menu, select File -\u0026gt; Upload Local Files\u0026hellip;, choose files for upload..   When the upload is complete, you can check the uploaded file in Cloud9.\n  Grant execute permission to the file. Enter the following into CLoud9\u0026rsquo;s terminal.\nsudo chmod 755 ./fail_instance.sh sudo chmod 755 ./fail_az.sh sudo chmod 755 ./failover_rds.sh   Make sure that jq is installed in Cloud9 (requires version 1.4 or higher).\n$ jq --version jq-1.5-1-a5b5cbe   If it is not installed, enter the following into CLoud9\u0026rsquo;s terminal.\nsudo yum install jq -y   If you type jq \u0026ndash;version again as above, you are ready.\n        Availability Zones (AZs) are isolated sets of resources within a region, each with redundant power, networking, and connectivity, housed in separate facilities. Each Availability Zone is isolated, but the Availability Zones in a Region are connected through low-latency links. AWS provides you with the flexibility to place instances and store data across multiple Availability Zones within each AWS Region for high resiliency.   Learn more: After the lab see this whitepaper on regions and availability zones      "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/reliability/test_resiliency/",
	"title": "Testing for Resiliency of EC2, RDS, and AZ",
	"tags": ["test_resiliency"],
	"description": "Use code to inject faults simulating EC2, RDS, and Availability Zone failures. These are used as part of Chaos Engineering to test workload resiliency",
	"content": "    Authors  Rodney Lester, Senior Solutions Architect Manager, AWS Well-Architected Adrian Hornsby, Principal Tech Evangelist, AWS Seth Eliot, Principal Reliability Solutions Architect, AWS Well-Architected +Seyong Kang, Solutions Architect  Introduction The purpose if this lab is to teach you the fundamentals of using tests to ensure your implementation is resilient to failure by injecting failure modes into your application. This may be a familiar concept to companies that practice Failure Mode Engineering Analysis (FMEA). It is also a key component of Chaos Engineering, which uses such failure injection to test hypotheses about workload resiliency. One primary capability that AWS provides is the ability to test your systems at a production scale, under load.\nIt is not sufficient to only design for failure, you must also test to ensure that you understand how the failure will cause your systems to behave. The act of conducting these tests will also give you the ability to create playbooks how to investigate failures. You will also be able to create playbooks for identifying root causes. If you conduct these tests regularly, then you will identify changes to your application that are not resilient to failure and also create the skills to react to unexpected failures in a calm and predictable manner.\nIn this lab, you will deploy a 3-tier resource, with a reverse proxy (Application Load Balancer), Web Application on Amazon Elastic Compute Cloud (EC2), and MySQL database using Amazon Relational Database Service (RDS). There is also an option to deploy the same stack into a different region, which provides you the ability to progress from simpler component failure testing to failure testing under a simulated AWS regional failure.\nThe skills you learn will help you build resilient workloads in alignment with the AWS Well-Architected Framework\nGoals:  Reduce fear of implementing resiliency testing by providing examples in common development and scripting languages Resilience testing of EC2 instances Resilience testing of RDS Multi-AZ instances Resilience testing using Availability Zones failures Resilience testing of S3 objects Learn how to implement resiliency using those tests Learn how to think about what a failure will cause within your infrastructure Learn how common AWS services can reduce mean time to recovery (MTTR)  Steps:  Preparation for Failure Injection   Test Resiliency Using EC2 Failure Injection   Test Resiliency Using RDS Failure Injection   Test Resiliency Using Availability Zone (AZ) Failure Injection   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/200_aws_resource_optimization/3_attach_iamrole/",
	"title": "Attach CloudWatch IAM role to selected EC2 Instances",
	"tags": [],
	"description": "",
	"content": "  We are now going to attach the IAM Role created on the previous step in one of our EC2 Instances, to do that let\u0026rsquo;s go to the Amazon EC2 Dashboard.   On the left bar, click on Instances.   Click on Launch Instance and select Linux 2 AMI (HVM) and t2.micro (free tier eligible) on the following screens. Click Review and launch:   Look for the created IAM role CloudWatchAgentServerRole under the IAM role box, select it and apply.   Confirm that the CloudWatchAgentServerRole was sucessfully attached to your EC2 instance   Validade if the IAM role CloudWatchAgentServerRole is attached to the desired instance   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/security/200_cloudfront_with_waf_protection/2_config_cloudfront/",
	"title": "CloudFront Config - EC2 or Load Balancer",
	"tags": [],
	"description": "",
	"content": "  Open AWS CloudFront Console: https://console.aws.amazon.com/cloudfront/home.\n  Click Create Distribution button.   in Web section, Click Get Started button.   Open new browser, open CloudFormation console: https://console.aws.amazon.com/cloudformation/ And click MasterAccount stack`s Output tab and copy value of ALBDNS\n  Enter the DNS address of the Application Load Balancer copied to Origin Domain Name in Origin Settings. Origin ID will be entered automatically.   If you scroll down, you can select the well-architected-cf-waf-acl you created earlier in the AWS WAF Web ACL in Distribution Setting. Click Create Distribution at the bottom to complete CloudFront setup.\n  Now you can check the distribution status of CloudFront being in progress (1) on the main screen of CloudFront. And when the deployment to the edge is completed, you can access the new Domain Name (2).   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/setup/wellarchitectedtools/3_perform_review/",
	"title": "Performing a review",
	"tags": [],
	"description": "",
	"content": "  From the detail page for the workload, click the Start review button. And click the AWS Well-Architected Framework.   In this walkthrough, we are only going to complete the Reliability Pillar questions. Collapse the Operational Excellence questions by selecting the collapsing icon on the left of the words Operational Excellence on the left:   Expand the Reliability Questions by selecting the expanding icon to the left of the word Reliability:   Select the first question: REL 1. How do you manage service limits?   Answer the REL 1 to REL 9 questions as you understand your current ability. You can use the Info links to help you understand what the answers mean, and watch the video to get more context on the questions. If the question does not apply to your workload, you can also skip the question by activating the Question does not apply to workload button.   If you can apply the question to your workload, but there are no details that apply, please select Not applicable at the bottom. If you see Done next to a question, the answer to that question is complete.   By default, the deployed environment doesn\u0026rsquo;t take into account anything, so you won\u0026rsquo;t have much to check. You\u0026rsquo;ll need to read the original questionnaire and answer about the state of your architecture, but to proceed with today\u0026rsquo;s lab, choose None for all questions and skip ahead. As you complete the question, select the Next Button at the bottom of the answers:   When you get to the last Reliability question, or the first Performance Pillar question, select Save and Exit:   If you chose Not Applicable for all of your questions, the following risks would have been created.   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/reliability/test_resiliency/4_failure_injection_ec2/",
	"title": "Test Resiliency Using EC2 Failure Injection",
	"tags": [],
	"description": "",
	"content": "4.1 EC2 failure injection This failure injection will simulate a critical problem with one of the three web servers used by your service.\n  Open EC2 Console, Select Instances in left menu: http://console.aws.amazon.com/ec2.\n  There are two EC2 instances with a name beginning with WellArchitectedLabsStack/ASG. For these EC2 instances note:\n Each has a unique Instance ID There is one instance per each Availability Zone All instances are healthy     Open up two more console in separate tabs/windows. From the left pane, open Target Groups and Auto Scaling Groups in separate tabs. You now have three console views open\n  To fail one of your EC2 instances, run the script below using your VPC ID in Cloud9\u0026rsquo;s terminal. Replace \u0026lt;vpc-id\u0026gt; with the VPC ID you copyed before.\n./fail_instance.sh \u0026lt;vpc-id\u0026gt;   The specific output will vary based on the command used, but will include a reference to the ID of the EC2 instance and an indicator of success. Here is the output for the Bash command. Note the CurrentState is shutting-down\n $ ./fail_instance.sh vpc-04f8541d10ed81c80 Terminating i-0710435abc631eab3 { \u0026quot;TerminatingInstances\u0026quot;: [ { \u0026quot;CurrentState\u0026quot;: { \u0026quot;Code\u0026quot;: 32, \u0026quot;Name\u0026quot;: \u0026quot;shutting-down\u0026quot; }, \u0026quot;InstanceId\u0026quot;: \u0026quot;i-0710435abc631eab3\u0026quot;, \u0026quot;PreviousState\u0026quot;: { \u0026quot;Code\u0026quot;: 16, \u0026quot;Name\u0026quot;: \u0026quot;running\u0026quot; } } ] }    Go to the EC2 Instances console which you already have open (or click here to open a new one)\n  Refresh it. (Note: it is usually more efficient to use the refresh button in the console, than to refresh the browser)\n  Observe the status of the instance reported by the script. In the screen cap below it is shutting down as reported by the script and will ultimately transition to terminated.\n    4.2 System response to EC2 instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n4.2.1 System availability Refresh the service website several times: For the website\u0026rsquo;s address, use ALBDNS in the Outputs of the CloudFormation MasterAccount Stack created in the lab setup.\n Website remains available The remaining two EC2 instances are handling all the requests (as per the displayed instance_id)  4.2.2 Load balancing Load balancing ensures service requests are not routed to unhealthy resources, such as the failed EC2 instance.\n  Go to the Target Groups console you already have open (or click here to open a new one)\n If there is more than one target group, select the one with the Load Balancer named ResiliencyTestLoadBalancer    Click on the Targets tab and observe:\n  Status of the instances in the group. The load balancer will only send traffic to healthy instances.\n  When the auto scaling launches a new instance, it is automatically added to the load balancer target group.\n  In the screen cap below the unhealthy instance is the newly added one. The load balancer will not send traffic to it until it is completed initializing. It will ultimately transition to healthy and then start receiving traffic.\n  Note the new instance was started in the same Availability Zone as the failed one. Amazon EC2 Auto Scaling automatically maintains balance across all of the Availability Zones that you specify.\n    From the same console, now click on the Monitoring tab and view metrics such as Unhealthy hosts and Healthy hosts\n![TargetGroupsMonitoring](/images/reliability/reliability-ec2-montoring.png)    4.2.3 Auto scaling Autos scaling ensures we have the capacity necessary to meet customer demand. The auto scaling for this service is a simple configuration that ensures at least three EC2 instances are running. More complex configurations in response to CPU or network load are also possible using AWS.\n  Go to the Auto Scaling Groups console you already have open (or click here to open a new one)\n If there is more than one auto scaling group, select the one with the name that starts with WebServersforResiliencyTesting    Click on the Activity History tab and observe:\n You can check instance status and time.     Draining allows existing, in-flight requests made to an instance to complete, but it will not send any new requests to the instance. Learn more: After the lab see this blog post for more information on draining.\nLearn more: After the lab see Auto Scaling Groups to learn more how auto scaling groups are setup and how they distribute instances, and Dynamic Scaling for Amazon EC2 Auto Scaling for more details on setting up auto scaling that responds to demand\n4.2.4 EC2 failure injection - conclusion Deploying multiple servers and Elastic Load Balancing enables a service suffer the loss of a server with no availability disruptions as user traffic is automatically routed to the healthy servers. Amazon Auto Scaling ensures unhealthy hosts are removed and replaced with healthy ones to maintain high availability.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/100_aws_resource_optimization/2_resource_opt/",
	"title": "Using Amazon EC2 Resource Optimization Recommendations",
	"tags": [],
	"description": "",
	"content": "NOTE: In order to complete this step you need to have Amazon EC2 Resource Optimization enabled, you can do that by going to AWS Cost Explorer, Recommendations (left bar) section.\nAmazon EC2 Resource Optimization offers right sizing recommendations in AWS Cost Explorer without any additional cost. These recommendations identify likely idle and underutilized instances across your accounts, regions and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (using Amazon CloudWatch) and your existing reservation footprint to identify opportunities for cost savings (e.g., by terminating idle instances or downsizing active instances to lower-cost options within the same family/generation).\n  Navigate to the AWS Cost Explorer page   Select Recommendations in the left menu bar   Click on the View All link associated with the Amazon EC2 Resource Optimization Recommendations section.   In case you haven’t enabled the Amazon EC2 Resource Optimization please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or payer accounts can enable it and by default both linked and payer accounts will be able to access their rightsizing recommendations unless the payer account specifically prohibits it on the settings page (top right). To improve the quality of recommendations, AWS might use other utilization metrics that you might be collecting, such as disk or memory utilization. All resource utilization metrics are anonymized and aggregated before AWS uses them for model training. If you would like to opt out of this experience and request your metrics not be stored and used for model improvement, please submit an AWS support ticket. For more information, see AWS Service Terms.\nAssuming you had enabled the Amazon EC2 Resource Optimization Recommendations, you will be presented with a screen that provides recommendations (if any exists):    Optimization opportunities – The number of recommendations available based on your resources and usage Estimated monthly savings – The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%) – The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list  You can also filter your recommendations by the type of action (Idle and Underutilized), Linked Account, Region and Tag.\nClick on view next to a recommendation, to view the details:   How are the potential savings calculated? AWS will first examine the instance running during the last 14 days to identify if it was partially or fully covered by an RI or running On-Demand. Another factor is whether the RI is instance size-flexible. The cost to run the instance is calculated based on the On-Demand hours and the hourly rate for the instance type. For each recommendation, AWS calculates the cost to operate a new instance. AWS assumes that an instance size-flexible Reserved Instance will cover the new instance in the same way as the previous instance. Savings are calculated based on the number of On-Demand running hours and the difference in On-Demand rates. If the Reserved Instance isn\u0026rsquo;t instance size-flexible, the savings calculation is based on whether the instance hours during the last 14 days are operated as On-Demand. AWS will only provide recommendations with estimated savings greater than or equal to $0.\nAmazon EC2 Resource Optimization recommendations already excludes Spot usage and takes into consideration the existing Reserved Instances and Savings Plan footprint to provide recommendations. There are two types of recommended actions: Terminate if the instance is considered idle (max CPU utilization is at or below 1%) or Downsize if the instance is underutilized (max CPU utilization is between 1% and 40%).\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/200_aws_resource_optimization/4_memory_plugin/",
	"title": "Cloudwatch Agent Manual Install",
	"tags": [],
	"description": "",
	"content": "  We are now going to manually install the CloudWatch agent to start collecting memory data, to start let\u0026rsquo;s go back to the Amazon EC2 Dashboard.   Check EC2 connect infomation which CloudWatchAgentServerRole IAM role attached. Select EC2 instance which attached IAM role and Click Connect button.   Copy connect info in Connect to your instance   This Command Line need when connect bastionhost. copy to notepad.\n EC2 are in PrivateSubnet, they cannot be accessed directly from outside the VPC.Use BastionHost to access EC2. Let\u0026rsquo;s copy the pem key used by EC2 to bastionhost.  You need to change the permissions of the .pem file. Enter chmod 400 \u0026lt;path.pem\u0026gt; to change the file permissions.     Enter the command below at the location of the pem key on the terminal.\nssh -i \u0026lt;pem path\u0026gt; \u0026lt;pem filename\u0026gt; ec2-user@\u0026lt;bastionhost-public-dns\u0026gt;:~ ``\n  pem-path : Enter the path of the key pair(.pem) used by EC2.\n  pem-filename : Enter the name of the pem file to the copy location. (Enter the same as before.)\n  bastionhost-public-dns : Click on CloudFormation\u0026rsquo;s output and you can see bastionhostdns. Or, you can check the public dns of bastionhost of the ec2 instance.\n  Find it on the EC2 dashboard   Find it on Cloudformation output     Example) Please enter in the form below.\nscp -i ./warworkshopkey.pem warworkshopkey.pem ec2-user@ec2-3-210-197-135.compute-1.amazonaws.com:~ ``\n  Upload is completed with the following message.   Open EC2 Dashboard, go to Instances and Click EC2 instance which name BastionHost. Click Connect button.   Click browser-based SSH connection tool in the newly opened dialog box, click ʻec2-user` as the User name, and click Connect.   If you type ls in the terminal, you can check the copied pem.   You can access the EC2 by pasting the copied EC2 connection information. You can check the changed ip of ec2-user in terminal shell Now, let\u0026rsquo;s install Cloudwatch-agent in earnest.   Download the Amazon Cloudwatch agent package, the instructions below are for Amazon Linux, for other OS please check here\nwget https://s3.amazonaws.com/amazoncloudwatch-agent/linux/amd64/latest/AmazonCloudWatchAgent.zip ``\n  Unzip and Install the package\nunzip AmazonCloudWatchAgent.zip sudo ./install.sh ``\n  Configure the AmazonCloudWatchAgent profile\nBefore running the CloudWatch agent on any servers, you must create a CloudWatch agent configuration file, which is a JSON file that specifies the metrics and logs that the agent is to collect, including custom metrics. You can create it by using the wizard or by writting it yourself from scratch. Any time you change the agent configuration file, you must then restart the agent to have the changes take effect.\nThe wizard can autodetect the credentials and AWS Region to use if you have the AWS credentials and configuration files in place. For more information about these files, see Configuration and Credential Files in the AWS Systems Manager User Guide and the AWS documentation page.\nFor now, let\u0026rsquo;s start the CloudWatch agent configuration file wizard executing the command below at the selected EC2 instance.\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-config-wizard ``\nFor this lab we want to keep the following structure:\n   CloudWatch Agent Configutation File Wizard Parameter     On which OS are you planning to use the agent? 1. Linux   Are you using EC2 or On-Premises hosts? 1. EC2   Which user are you planning to run the agent? 2. cwagent   Do you want to turn on StatsD daemon? 2. No   Do you want to monitor metrics from CollectD? 2. No   Do you want to monitor any host metrics? 1. Yes   Do you want to monitor cpu metrics per core? 2. No   Do you want to add ec2 dimensions? 1. Yes   Would you like to collect your metrics at high resolution? 4. 60s   Which default metrics config do you want? 1. Basic   Are you satisfied with the above config? 1. Yes   Do you have any existing CloudWatch Log Agent? 2. No   Do you want to monitor any log files? 2. No   Do you want to store the config in the SSM parameter store? 2. No    The CloudWatch Agent config file should look like the following:\n{ \u0026quot;agent\u0026quot;: { \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;run_as_user\u0026quot;: \u0026quot;cwagent\u0026quot; }, \u0026quot;metrics\u0026quot;: { \u0026quot;append_dimensions\u0026quot;: { \u0026quot;AutoScalingGroupName\u0026quot;: \u0026quot;${aws:AutoScalingGroupName}\u0026quot;, \u0026quot;ImageId\u0026quot;: \u0026quot;${aws:ImageId}\u0026quot;, \u0026quot;InstanceId\u0026quot;: \u0026quot;${aws:InstanceId}\u0026quot;, \u0026quot;InstanceType\u0026quot;: \u0026quot;${aws:InstanceType}\u0026quot; }, \u0026quot;metrics_collected\u0026quot;: { \u0026quot;disk\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60, \u0026quot;resources\u0026quot;: [ \u0026quot;*\u0026quot; ] }, \u0026quot;mem\u0026quot;: { \u0026quot;measurement\u0026quot;: [ \u0026quot;mem_used_percent\u0026quot; ], \u0026quot;metrics_collection_interval\u0026quot;: 60 } } } } ``\n  Start the CloudWatch Agent\nsudo /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/bin/config.json -s ``\nIt may take up to 5 minutes for the metrics to become available, go back to the Amazon CloudWatch console page, under the Metrics session to validate that you are getting Memory information.\nClick CWAgent: Click ImageID,InstanceID,InstanceType: Select the Instance from the list below: You have now completed the CloudWatch agent installation and will be able to monitor on Amazon CloudWatch the memory utilization of that instance.\n  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/100_aws_resource_optimization/3_prio_resource_opt/",
	"title": "Download the Amazon EC2 Resource Optimization CSV File and sort it to find quick wins",
	"tags": [],
	"description": "",
	"content": "  Download the Amazon EC2 Resource Optimization report:   If you don’t have any Amazon EC2 Resource Optimization recommendation use the file below as a reference. Sample Amazon EC2 Resource Optimization file (.xlsx)\n  First let’s exclude instances that are too small or were only running for a few hours from the analysis. By doing so, we minimize the time required to perform rightsizing modifications that would otherwise result in minimal savings.   NOTE: Depending on how many cost allocation tags you have enabled on your account the columns may differ from the example, that said try to match the formulas using the screenshots below and the default column names.\nInsert a new column to the right of the Recommended Action column. The first row will be the label for the column “TooSmall”. In each row below the label, paste the following formula:\n=IF(Q2\u0026lt;25,1,0) Where Column Q = Recommended Instance Type 1 Estimated Savings\nThat formula will flag all EC2 instances with a “1” any instance that will fail to deliver more than $25/month in savings (or $300/year). Feel free to adjust the threshold for your organization own savings expectation. If you prefer to perform the analysis against instance families instead of potential savings you can use the following formula to exclude smaller instances from the recommendations as well.\n=IF(N2=\u0026quot;Modify\u0026quot;,IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\u0026quot;nano\u0026quot;,\u0026quot;micro\u0026quot;,\u0026quot;small\u0026quot;,\u0026quot;medium\u0026quot;},D2)))))\u0026gt;0,\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;),\u0026quot;0\u0026quot;) Where Column N = Recommended Action and Column D = Instance Type\nNext, let’s flag EC2 instances that belong to previous generations (C4, M3, etc), if you are investing engineer time on right sizing let\u0026rsquo;s make sure you are also leveraging the newest technology available. Newer EC2 generations have a superior performance increasing the changes of success for the right sizing exercise, they also generally cost less than previous generations providing a higher cost vs benefit.  Insert a new column Old Gen to the right of the Instance Type field, and paste the following formula:\n=IF(SUMPRODUCT(--(NOT(ISERR(SEARCH({\u0026quot;c4\u0026quot;,\u0026quot;c3\u0026quot;,\u0026quot;c1\u0026quot;,\u0026quot;m4\u0026quot;,\u0026quot;m3\u0026quot;,\u0026quot;m2\u0026quot;,\u0026quot;m1\u0026quot;,\u0026quot;r3\u0026quot;,\u0026quot;r4\u0026quot;,\u0026quot;i2\u0026quot;,\u0026quot;cr1\u0026quot;,\u0026quot;hs1\u0026quot;,\u0026quot;g2\u0026quot;},D2)))))\u0026gt;0,\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;) Column D = Instance Type\nNow let’s sort the recommendations by low complexity and higher savings:  Minimum Effort: Set the minimum savings required\nFirst we want to only focus on savings that are worth our effort, we will define this as $100. Apply a number filter on Recommended Instance Type 1 Estimated Savings that is Greater than 100\nGroup 1: Idle EC2 resources\nFilter the data on Recommended Action = \u0026ldquo;Terminate\u0026rdquo;\nSort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest\nStart filtering the idle resources or instances where CPU utilization \u0026lt;1%, it is likely these instances were launched and forgotten so the potential savings here may represent the entire On Demand cost.\nThe resulting filtered list should be where you start right sizing discussions with application owners; perform an investigation to understand why these instance were launched and validate their usage with the resource owner. If possible, terminate them.\nIf you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from an original 2,534 recommendations to 16 and identified $3,458 per month in potential savings.\nGroup 2: Previous generation instances\nFilter the data on Recommended Actions = “Modify” AND OldGen = “1” AND TooSmall = “0”\nFilter the data on Recommended Instance Type 1 Projected CPU \u0026lt; 40%\nSort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest\nThis will focus on the underutilized resources (\u0026lt;40% CPU) that belongs to previous generations and can either be downsized within the same family (column P below) or modernized to the newest generation.\nMoving to a modern generation may require additional testing hours compared to instances identified on Group 1, but depending on the case it can maximize savings and performance.\n    Linux vs new gen Windows vs new gen  Linux vs new gen Windows vs new gen     c3.large $0.105/hr up 19% $0.188/hr up 5% m3.large $0.133/hr up 27% $0.259/hr up 27%   c4.large $0.100/hr up 15% $0.192/hr up 7% m4.large $0.100/hr up 4% $0.192/hr up 2%   c5.large $0.985/hr 0% $0.177/hr 0% m5.large $0.096/hr 0% $0.188/hr 0%    prices are from US-Virginia (Nov 2019)\nIf you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 2,534 recommendations to 22 with $6,362 per month in potential savings.\nGroup 3: Current generation instances\nFilter the data on Recommended Actions = “Modify” AND OldGen = “0” AND TooSmall = “0”\nThe filter on Recommended Instance Type 1 Projected CPU \u0026lt; 40% should still be in place.\nSort the data by Recommended Instance Type 1 Estimated Savings = Largest to smallest\nThis will select underutilized resources from the current, most modern generation. We recommend sorting them by potential savings to make sure you are prioritizing the instances that will provide larger savings first.\nAlso, do not forget to check the other recommended instance types (columns U to AD); Amazon EC2 Resource Optimization will recommend up to 3 instances for each resource moving from a more conservative recommendation (the first recommendation) to a more aggressive and higher savings recommendation (second and third recommendations).\nIf you are using the Right Sizing CSV file provided in this lab exercise, you will notice that we filtered down from originally 2,534 recommendations to 22 with $4,879.56 per month in potential savings.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/setup/wellarchitectedtools/4_save_milestone/",
	"title": "Saving a milestone",
	"tags": [],
	"description": "",
	"content": "  From the detail page for the workload, click the Save milestone button:   Enter a name for the milestone as AWS Workshop Milestone and click the Save button:   Click on the Milestones tab:   This will display the milestone and data about it:   Now let\u0026rsquo;s change the state of the workload. Click on the Workload for AWS Workshop workload in the left navigation bar, and select the Properties tab. And if you scroll down, change the Workload status from Workload status to In Progress.   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/reliability/test_resiliency/5_failure_injection_rds/",
	"title": "Test Resiliency Using RDS Failure Injection",
	"tags": [],
	"description": "",
	"content": "5.1 RDS failure injection This failure injection will simulate a critical failure of the Amazon RDS DB instance.\n  Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds\n  From the RDS dashboard\n Click on \u0026ldquo;DB Instances (n/40)\u0026rdquo; Click on the DB identifier for your database (if you have more than one database, refer to the VPC ID to find the one for this workshop) If running the multi-region deployment, select the DB instance with Role=Master Select the Configuration tab    Look at the configured values. Note the following:\n Value of the Info field is Available RDS DB is configured to be Multi-AZ. The primary DB instance and the standby DB instance is located diffrent AZ.     To fail one of your RDS instances, run the script below using your VPC ID in Cloud9\u0026rsquo;s terminal. Replace \u0026lt;vpc-id\u0026gt; with the VPC ID you copyed before.\n./failover_rds.sh \u0026lt;vpc-id\u0026gt;   The specific output will vary based on the command used, but will include some indication that the your Amazon RDS Database is being failedover: Failing over mdk29lg78789zt\n  5.2 System response to RDS instance failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n5.2.1 System availability   The website is not available. Some errors you might see reported:\n No Response / Timeout: Request was successfully sent to EC2 server, but server no longer has connection to an active database 504 Gateway Time-out: Amazon Elastic Load Balancer did not get a response from the server. This can happen when it has removed the servers that are unable to respond and added new ones, but the new ones have not yet finished initialization, and there are no healthy hosts to receive the request 502 Bad Gateway: The Amazon Elastic Load Balancer got a bad request from the server An error you will not see is This site can’t be reached. This is because the Elastic Load Balancer has a node in each of the three Availability Zones and is always available to serve requests.    Continue on to the next steps, periodically returning to attempt to refresh the website.\n  5.2.2 Failover to standby   On the database console Configuration tab\n  Refresh and note the values of the Info field. It will ultimately return to Available when the failover is complete.\n  Note the AZs for the primary and standby instances. They have swapped as the standby has no taken over primary responsibility, and the former primary has been restarted. (After RDS failover it can take several minutes for the console to update as shown below. The failover has however completed)\n  From the AWS RDS console, click on the Logs \u0026amp; events tab and scroll down to Recent events. You should see entries like those below. In this case failover took less than a minute.\n    Mon, 14 Oct 2019 19:53:37 GMT - Multi-AZ instance failover started. Mon, 14 Oct 2019 19:53:45 GMT - DB instance restarted Mon, 14 Oct 2019 19:54:21 GMT - Multi-AZ instance failover completed     5.2.3 RDS failure injection - conclusion  AWS RDS Database failover took less than a minute   Resources Learn more: After the lab see High Availability (Multi-AZ) for Amazon RDS for more details on high availability and failover support for DB instances using Multi-AZ deployments.\nHigh Availability (Multi-AZ) for Amazon RDS\n The primary DB instance switches over automatically to the standby replica if any of the following conditions occur:\n An Availability Zoßne outage The primary DB instance fails The DB instance\u0026rsquo;s server type is changed The operating system of the DB instance is undergoing software patching A manual failover of the DB instance was initiated using Reboot with failover   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/100_aws_resource_optimization/4_act_resource_opt/",
	"title": "Action the recommendations",
	"tags": [],
	"description": "",
	"content": "During this lab exercise, we learned how to prioritize the right sizing recommendations with the goal of identifying low complexity and high savings recommendations. We initially started with 2,534 recommendations with a potential saving of $86,627 but we managed to identify the top 60 cases with lowest complexity that together add up to $14,699.56 of the overall potential saving.\nGroup 1 (Idle) and Group 2 (Previous Generation) are the less complex cases where you may want to start the right sizing exercises for your organization. As you gain more confidence and learn how to develop a regular process for right sizing, your organization will be able to rapidly act on Group 3 (Current/modern generation) and other cases.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/reliability/test_resiliency/6_failure_injection_az/",
	"title": "Test Resiliency Using Availability Zone (AZ) Failure Injection",
	"tags": [],
	"description": "",
	"content": "6.1 AZ failure injection This failure injection will simulate a critical problem with one of the three AWS Availability Zones (AZs) used by your service. AWS Availability Zones are powerful tools for helping build highly available applications. If an application is partitioned across AZs, companies are better isolated and protected from issues such as lightning strikes, tornadoes, earthquakes and more.\n Go to the RDS Dashboard in the AWS Console at http://console.aws.amazon.com/rds and note which Availability Zone the AWS RDS primary DB instance is in.  Note: If you previously ran the RDS Failure Injection test, you must wait until the console shows the AZs for the primary and standby instances as swapped, before running this test A good way to run the AZ failure injection is first in an AZ other than this - we\u0026rsquo;ll call this Scenario 1 Then try it again in the same AZ as the AWS RDS primary DB instance - we\u0026rsquo;ll call this Scenario 2 Taking down two out of the three AZs this way is an unlikely use case, however it will show how AWS systems work to maintain service integrity despite extreme circumstances. And executing this way illustrates the impact and response under the two different scenarios.    This lab example is us-east-2, if you use another region, consider region and AZ name.\n   To simulate failure of an AZ, select one of the Availability Zones used by your service (us-east-2a, us-east-2b, or us-east-2c) as \u0026lt;az\u0026gt;\n For scenario 1 select an AZ that is neither primary nor secondary for your RDS DB instance. Given the following RDS console you would choose us-east-2c For scenario 2 select the AZ that is primary for your RDS DB instance. Given the following RDS console you would choose us-east-2b    To fail one of your AZ, run the script below using your VPC ID in Cloud9\u0026rsquo;s terminal. Replace \u0026lt;vpc-id\u0026gt; with the VPC ID you copyed before. and replace \u0026lt;az\u0026gt; with the your az.\n  ./failover_az.sh \u0026lt;az\u0026gt; \u0026lt;vpc-id\u0026gt;  The specific output will vary based on the command used.  Note whether an RDS failover was initiated. This would be the case if you selected the AZ containing the AWS RDS primary DB instance    6.2 System response to AZ failure Watch how the service responds. Note how AWS systems help maintain service availability. Test if there is any non-availability, and if so then how long.\n6.2.1 System availability Refresh the service website several times\n Scenario 1: If you selected an AZ not containing the AWS RDS primary DB instance then you should see uninterrupted availability Scenario 2: If you selected the AZ containing the AWS RDS primary DB instance, then an availability loss similar to what you saw with RDS fault injection testing will occur.  6.2.2 Scenario 1 - Load balancer and web server tiers This scenario is similar to the EC2 failure injection test because there is only one EC2 server per AZ in our architecture. Look at the same screens you as for that test:\n EC2 Instances Load Balancer Target group Auto Scaling Groups  One difference from the EC2 failure test that you will observe is that auto scaling will bring up the replacement EC2 instance in an AZ that already has an EC2 instance as it attempts to balance the requested three EC2 instances across the remaining AZs.\n6.2.3 Scenario 2 - Load balancer, web server, and data tiers This scenario is similar to a combination of the RDS failure injection along with EC2 failure injection. In addition to the EC2 related screens look at the Amazon RDS console, navigate to your DB screen and observe the following tabs:\n Configuration Monitoring Logs \u0026amp; Events  6.2.4 AZ failure injection - conclusion This similarity between scenario 1 and the EC2 failure test, and between scenario 2 and the RDS failure test is illustrative of how an AZ failure impacts your system. The resources in that AZ will have no or limited availability. With the strong partitioning and isolation between Availability Zones however, resources in the other AZs continue to provide your service with needed functionality. Scenario 1 results in loss of the load balancer and web server capabilities in one AZ, while Scenario 2 adds to that the additional loss of the data tier. By ensuring that every tier of your system is in multiple AZs, you create a partitioned architecture resilient to failure.\n6.2.5 AZ failure recovery This step is optional. To simulate the AZ returning to health do the following:\n Go to the Auto Scaling Group console Select the WebServersforResiliencyTesting auto scaling group Actions \u0026raquo; Edit In the Subnet field add any ResiliencyVPC-PrivateSubnets that are missing (there should be three total) and Save Go to the Network ACL console Look at the NACL entries for the VPC called ResiliencyVPC For any of these NACLs that are not Default do the following  Select the NACL Actions \u0026raquo; Edit subnet associations Uncheck all boxes and click Edit Actions \u0026raquo; Delete network ACL     Note how the auto scaling redistributes the EC2 serves across the availability zones  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/200_aws_resource_optimization/5_ec2_updated_rec/",
	"title": "Updated Amazon EC2 Resource Optimization recommendations",
	"tags": [],
	"description": "",
	"content": " Please try this exercise in a real user account. Currently, it cannot be performed in a lab environment.\n In order to complete this step you need to have Amazon EC2 Resource Optimization enabled, you can do that going to the AWS Cost Explorer, Recommendations (left bar) section.\n If you have just installed the CloudWatch agent at your instances it may take a couple of days for Amazon EC2 Resource Optimization to start to provide updated recommendations, so don\u0026rsquo;t worry if you don\u0026rsquo;t see the memory data during the first checks.\n Now that we have Memory data as a custom metric in CloudWatch let\u0026rsquo;s check how that affects the Amazon EC2 Resource Optimization recommendations.\nAmazon EC2 Resource Optimization offers right sizing recommendations in the AWS Cost Explorer without any additional cost. These recommendations identify likely idle and underutilized instances across your accounts, regions and tags. To generate these recommendations, AWS analyzes your historical EC2 resource usage (last 14 days using Amazon CloudWatch) and your existing reservation footprint to identify opportunities for cost savings. There are two types of recommended actions: Terminate if the instance is considered idle (max CPU utilization is at or below 1%) or Downsize if the instance is underutilized (max CPU utilization is between 1% and 40%).\nBy default Amazon EC2 Resource Optimization doesn\u0026rsquo;t need memory datapoint to provide recommendations, but if that information is available it will take that into consideration updating the Downsize recommendation for instances that now have max CPU and MEM utilization between 1% and 40% over the past 14 days. Let\u0026rsquo;s validate that with the following steps.\n  Navigate to the AWS Cost Explorer page   Select Recommendations in the left menu bar   Click on the View All link associated with the Amazon EC2 Resource Optimization Recommendations section. In case you haven’t enabled the Amazon EC2 Resource Optimization please do so (no additional cost), it may take up to 24 hours in order to generate your first recommendations. Only regular or payer accounts can enable it and by default both linked and payer accounts will be able to access their rightsizing recommendations unless the payer account specifically prohibits it on the settings page (top right).\n  Assuming you had enabled the Amazon EC2 Resource Optimization Recommendations, you will be presented with a screen that provides recommendations (if any exists). Click to view the Resource Optimization recommendations.\n Optimization opportunities – The number of recommendations available based on your resources and usage Estimated monthly savings – The sum of the projected monthly savings associated with each of the recommendations provided Estimated savings (%) – The available savings relative to the direct Amazon EC2 costs (On-Demand) associated with the instances in the recommendation list  You can also filter your recommendations by the type of action (Idle and Underutilized), Linked Account, Region and Tag.\n  Understanding the Amazon EC2 Resource Optimization recommendations.\nIn the example below we have a recommendation to downsize the t2.micro (1vCPU for a 2h 24m burst and 1GB RAM) to a t2.nano (1vCPU for a 1h 12m burst and 0.5 GB RAM) and save $12 USD per year.\nIn the example below we have a recommendation to downsize the t2.micro (1vCPU for a 2h 24m burst and 1GB RAM) to a t2.nano (1vCPU for a 1h 12m burst and 0.5 GB RAM) and save $12 USD per year. Over the past 14 days the maximum CPU utilization for this instance was only 9% and this instance was running for 86 hours and all of these were On Demand hours. Observe that there is no memory information available so Amazon EC2 Resource Optimization will ignore that datapoint and recommend to downsize to a t2.nano that have half of the memory available of a t2.micro.\nThat can be risky and waste engineer time when testing if the proposed right sizing option is valid or not. That said you can improve the accuracy of this recommendation with the CloudWatch agent we just installed.\nIn this other example we see a recommendation to downsize a r5.8xlarge (32 vCPU and 256GB RAM) to a r5.4xlarge (16 vCPU and 128GB RAM) and save $2,412 USD per year. On this case we have both CPU and Memory information available: the maximum CPU utilization was 21% and Memory was only 5%. That makes the case for downsize much stronger and the recommendation will even try estimate the CPU and Memory utilization for the new instance size. Keep in mind that this is just a simple estimation based on the past utilization data from CloudWatch, before executing the modification all the required load tests must be performed to avoid any impacts on your workload.\nAs explained above the Amazon EC2 Resource Optimization logic will recommend to downsize any instances where the maximum CPU utilization was between 1% to 40% over the past 14 days. If you do have memory information available the Amazon EC2 Resource Optimization will now consider to downsize instances that have both CPU and Memory maximum utilization between 1% and 40%. Idle recommendations are not impacted if memory data is available, so any EC2 instance that during the past 14 days never passed 1% of CPU utlization will be automatically flagged as idle.\n  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/setup/wellarchitectedtools/5_view_report/",
	"title": "Viewing and downloading the report",
	"tags": [],
	"description": "",
	"content": "  From the detail page for the workload, click the Improvement Plan tab:   This will display the number of high and medium risk items and allow you to update the Improvement status:   You can also edit the Pillar priority configuration. Click Workload for AWS workshop in the left menu, scroll down and click the Edit button in the Pillar priority tab:   Move the Reliability Pillar up by clicking the up icon to the right of the word, Reliability:   Click the Save button to save this configuration:   Now let\u0026rsquo;s generate and download the report. Click AWS Well-Architected Framework in the AWS Workshop Milestone workload on the left navigation bar, then click Generate report to download the report:   You can now open or save the file to view. The report includes risk factors by pillar and improvement plans according to questions. To see the Improvement Plan in the console, click Continue review and scroll down to the bottom of the questionnaire.\n  Reports are generated in the set language.\n "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/100_aws_resource_optimization/5_ec2_rs_best_practices/",
	"title": "Amazon EC2 Right Sizing Best Practices",
	"tags": [],
	"description": "",
	"content": "  Start simple: idle resources, non-critical development/QA and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by).\n  Right Size before performing a migration: If you skip right sizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the “t” family.\n  The best right sizing starts on day 1: As you perform right sizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads and upcoming migrations.\n  Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load, or functioning incorrectly.\n  Test once and perform multiple right sizing: Aggregate instances per autoscaling group and tags to scale right sizing activities.\n  Combine Reserved Instance or Savings Plans strategy with Right Sizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings plan will automatically adjust the commitment for the new environment.\n  Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldn’t be part of the instance types being analyzed for rightsizing.\n  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/200_aws_resource_optimization/6_ec2_rs_best_practices/",
	"title": "Amazon EC2 Right Sizing Best Practices",
	"tags": [],
	"description": "",
	"content": "  Start simple: idle resources, non-critical development/QA and previous generation instances will require less testing hours and provide quick wins (The Amazon EC2 Launch time statistics can be used to identify instances that have been running longer than others and is a good statistic to sort your Amazon EC2 instances by).\n  Right Size before performing a migration: If you skip right sizing to save time, your migration speed might increase, but you will end up with higher cloud infrastructure spend for a potentially longer period of time. Instead, leverage the test and QA cycles during a migration exercise to test several instance types and families. Also, take that opportunity to test different sizes and burstable instances like the “t” family.\n  The best right sizing starts on day 1: As you perform right sizing analysis, and ultimately rightsize resources, ensure any learnings are being shared across your organization and influencing the design of new workloads.\n  Measure Twice, Cut Once: Test, then test some more: The last thing you want is for a new resource type to be uncapable of handling load, or functioning incorrectly.\n  Test once and perform multiple right sizing: Aggregate instances per autoscaling group and tags to scale right sizing activities.\n  Combine Reserved Instance or Savings Plans strategy with Right Sizing to maximize savings: For Standard RIs and EC2 Instance SP: Perform your pricing model purchases after rightsizing and for Convertible RIs, exchange them after rightsizing. Compute Savings plan will automatically adjust the commitment for the new environment.\n  Ignore burstable instance families (T types): These families are designed to typically run at low CPU percentages for significant periods of time and shouldn’t be part of the instance types being analyzed for rightsizing.\n  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/100_aws_resource_optimization/6_tear_down/",
	"title": "Tear down",
	"tags": [],
	"description": "",
	"content": "No tear down is required for this lab.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/200_aws_resource_optimization/7_tear_down/",
	"title": "Tear down",
	"tags": [],
	"description": "",
	"content": " Delete the IAM role CloudWatchAgentServerRole.  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/100_aws_resource_optimization/",
	"title": "EC2 Right Sizing",
	"tags": [],
	"description": "",
	"content": "Authors  Arthur Basbaum, AWS Cloud Economics  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will give you an overview on Amazon CloudWatch and AWS Resource Optimization and how to prioritize your EC2 Right Sizing efforts.\nGoals  Learn how to check metrics like CPU, Network and Disk usage on Amazon CloudWatch Enable and use the AWS Resource Optimization and get EC2 Right Sizing recommendations Learn how to filter AWS Resource Optimization report and focus only on the less complex high saving cases  Prerequisites  Root user access to the master account Enable AWS Resource Optimization at AWS Cost Explorer \u0026gt; Recommendations no additional cost.  Permissions required  NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Steps:  Getting to know Amazon Cloudwatch   Using Amazon EC2 Resource Optimization Recommendations   Download the Amazon EC2 Resource Optimization CSV File and sort it to find quick wins   Action the recommendations   Amazon EC2 Right Sizing Best Practices   Tear down   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/200_aws_resource_optimization/",
	"title": "EC2 Right Sizing(MEM)",
	"tags": [],
	"description": "",
	"content": "Authors  Jeff Kassel, AWS Technical Account Manager Arthur Basbaum, AWS Cloud Economics +Seyong Kang, Solutions Architect  Feedback If you wish to provide feedback on this lab, there is an error, or you want to make a suggestion, please email: costoptimization@amazon.com\nIntroduction This hands-on lab will guide you through the steps to install the CloudWatch agent to collect memory utilization (% GB consumption) and analyze how that new datapoint can help during EC2 right sizing exercises with the AWS Resource Optimization tool.\nGoals  Learn how to check metrics like CPU, Network and Disk usage on Amazon CloudWatch Learn how to install and collect Memory data through a custom metric at Amazon CloudWatch Enable AWS Resource Optimization and observe how the recommendations are impacted by this new datapoint (Memory)  Prerequisites  Root user access to the management account Enable AWS Resource Optimization at AWS Cost Explorer \u0026gt; Recommendations no additional cost.  Permissions required  Root user access to the management account NOTE: There may be permission error messages during the lab, as the console may require additional privileges. These errors will not impact the lab, and we follow security best practices by implementing the minimum set of privileges required.  Steps:  Getting to know Amazon Cloudwatch   Attach CloudWatch IAM role to selected EC2 Instances   Cloudwatch Agent Manual Install   Updated Amazon EC2 Resource Optimization recommendations   Amazon EC2 Right Sizing Best Practices   Tear down   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/security/200_cloudfront_with_waf_protection/",
	"title": "CloudFront with WAF Protection",
	"tags": [],
	"description": "",
	"content": "Authors  Ben Potter, Security Lead, Well-Architected +Seyong Kang, Solutions Architect  Introduction This hands-on lab will guide you through the steps to protect a workload from network based attacks using Amazon CloudFront and AWS Web Application Firewall (WAF). You will use the AWS Management Console and AWS CloudFormation to guide you through how to deploy CloudFront with WAF integration to apply defense in depth methods. Skills learned will help you secure your workloads in alignment with the AWS Well-Architected Framework.\nGoals  Protecting network and host-level boundaries System security configuration and maintenance Enforcing service-level protection  Steps:  AWS WAF Config   CloudFront Config - EC2 or Load Balancer   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/introduction/",
	"title": "Introduce",
	"tags": [],
	"description": "",
	"content": " if you understand What is Well Architected Framework, jump to step 2(setup).\n When you look at the system your team is building, can you answer the question: “Are you Well-Architected??” Every day, experts at AWS assist customers in architecting systems to take advantage of best practices in the cloud. We work with you on making architectural trade-offs as your designs evolve. As you deploy these systems into live environments, we learn how well these systems perform and the consequences of those trade-offs.\nBased on what we have learned, we have created the AWS Well-Architected Framework, which provides a consistent set of best practices for customers and partners to evaluate their architectures, and provides a set of questions you can use to evaluate how well an architecture is aligned to AWS best practices.\n What is AWS Well-Architected Framework?  "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/introduction/framework/",
	"title": "Why AWS-Well-Architected Framework?",
	"tags": [],
	"description": "",
	"content": "AWS Well-Architected Framework has a advantages.  Build and deploy faster Lower or mitigate risks Make informed decisions Learn AWS best practices   This framework is a mechanism. To drive better outcomes, you can build and operate workloads on the cloud. With Well-Architected built a mechanism:\nWell-Architected-Framework asks questions and works with the intention of improving the architecture. It also means replacing it with a perfect mechanism to check for compliance with rules or processes.\nit repeatable process that learn cloud service and best practice, measure architecture data, and improve architecture using it.\n Configuration AWS Well-Architected Framework The AWS Well-Architected Framework consists of:\n  Based on five pillar.\n Creating a software system is a lot like constructing a building. If the foundation is not solid, structural problems can undermine the integrity and function of the building. When architecting technology solutions, if you neglect the ﬁve pillars of operational excellence, security, reliability, performance efficiency, and cost optimization, it can become challenging to build a system that delivers on your expectations and requirements. Incorporating these pillars into your architecture will help you produce stable and efficient systems. This will allow you to focus on the other aspects of design, such as functional requirements.    General Design Principles\n  The Well-Architected Framework identifies a set of general design principles to facilitate good design in the cloud:\n Stop guessing your capacity needs Test systems at production scale Automate to make architectural experimentation easier Allow for evolutionary architectures Drive architectures using data Improve through game days       So let\u0026rsquo;s take a look at the five pillar of AWS Well-Architected.   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/introduction/pillar/",
	"title": "The Five Pillars of the Framework",
	"tags": [],
	"description": "",
	"content": "The AWS Well-Architected Framework is based on ﬁve pillars — operational excellence, security, reliability, performance efficiency, and cost optimization.\n   Pillar Description     Operational Excellence The ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures to deliver business value.   Security The security pillar encompasses the ability to protect data, systems, and assets to take advantage of cloud technologies to improve your security.   Reliability The reliability pillar encompasses the ability of a workload to perform its intended function correctly and consistently when it’s expected to. This includes the ability to operate and test the workload through its total lifecycle. This paper provides in-depth, best practice guidance for implementing reliable workloads on AWS.   Performance Efficiency The ability to use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve.   Cost Optimization The ability to run systems to deliver business value at the lowest price point.    When architecting workloads, you make trade-oﬀs between pillars based on your business context. These business decisions can drive your engineering priorities. You might optimize to reduce cost at the expense of reliability in development environments, or, for mission-critical solutions, you might optimize reliability with increased costs. In ecommerce solutions, performance can affect revenue and customer propensity to buy. Security and operational excellence are generally not traded-oﬀ against the other pillars.\n Let\u0026rsquo;s start each pillar hand on labs!   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/",
	"title": "Home",
	"tags": [],
	"description": "",
	"content": "Well Architected Labs  Introduction This lab guide not covers all best practices in pillar. Detail info is check AWS Well-Architected Framework and AWS Well-Architected Labs.\n  This lab includes Best-Practice that can be applied to basic 3-tier-web. This lab CloudFormation configuration for basic 3-tier-web is deploy. If you complete the Well-Architeced Framework review and practice for each pillar, you can complete a 3-tier-web applying best practices for each pillar. Finally, you can experience the mechanism by repeatedly by performing Well-Architected Framework Review again.  The Well-Architected framework has been developed to help cloud architects build the most secure, high-performing, resilient, and efficient infrastructure possible for their applications. This framework provides a consistent approach for customers and partners to evaluate their architectures, and provides guidance to help implement designs that will scale with your application needs over time.\nLicense Licensed under the Apache 2.0 and MITnoAttr License.\nCopyright 2018-2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\nLicensed under the Apache License, Version 2.0 (the \u0026ldquo;License\u0026rdquo;). You may not use this file except in compliance with the License. A copy of the License is located at\nhttps://aws.amazon.com/apache2.0/\nor in the \u0026ldquo;license\u0026rdquo; file accompanying this file. This file is distributed on an \u0026ldquo;AS IS\u0026rdquo; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n © 2020 Amazon Web Services, Inc. or Co, All rights reserved.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/setup/",
	"title": "Setup",
	"tags": [],
	"description": "",
	"content": " This CloudFormation stack include all enviromnent each pillar`s hands on labs.\n Enviroment Use Services    Name Description     Amazon EC2 Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It is designed to make web-scale cloud computing easier for developers. Amazon EC2’s simple web service interface allows you to obtain and configure capacity with minimal friction. It provides you with complete control of your computing resources and lets you run on Amazon’s proven computing environment.   Application Load Balancer A load balancer serves as the single point of contact for clients. The load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. This increases the availability of your application. You add one or more listeners to your load balancer.   Amazon RDS Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. It frees you to focus on your applications so you can give them the fast performance, high availability, security and compatibility they need.   AWS Auto Scaling AWS Auto Scaling monitors your applications and automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple resources across multiple services in minutes. The service provides a simple, powerful user interface that lets you build scaling plans for resources including Amazon EC2 instances and Spot Fleets, Amazon ECS tasks, Amazon DynamoDB tables and indexes, and Amazon Aurora Replicas. AWS Auto Scaling makes scaling simple with recommendations that allow you to optimize performance, costs, or balance between them. If you’re already using Amazon EC2 Auto Scaling to dynamically scale your Amazon EC2 instances, you can now combine it with AWS Auto Scaling to scale additional resources for other AWS services. With AWS Auto Scaling, your applications always have the right resources at the right time.    -Let\u0026rsquo;s deploy Cloud9 \u0026amp; CloudFomration stack.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/setup/cloud9/",
	"title": "Cloud9",
	"tags": [],
	"description": "",
	"content": "Let\u0026rsquo;s make Cloud9 instance: AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don’t need to install files or configure your development machine to start new projects. Since your Cloud9 IDE is cloud-based, you can work on your projects from your office, home, or anywhere using an internet-connected machine. Cloud9 also provides a seamless experience for developing serverless applications enabling you to easily define resources, debug, and switch between local and remote execution of serverless applications. With Cloud9, you can quickly share your development environment with your team, enabling you to pair program and track each other\u0026rsquo;s inputs in real time.\nCloud9 console start: https://console.aws.amazon.com/cloud9\nmake Workspace   select Create Environment on left menu.   typing WellArchitectedWorkshop's Cloud9 at Name environment of Name, at Description typing for Workshop and click Next Step.   Click Next step button.   Deploy Cloud9 to the default VPC. Cloud9 may shut down if it deploy in wellarchitectedlabs VPC when chasos test in a reliability pillar.\n   Review stage, check Cloud9 enviroment and setting click Create Environment.   Deploy finished you can use Cloud9.   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/setup/lab-setup-master/",
	"title": "CloudFormation Deploy",
	"tags": [],
	"description": "",
	"content": "AWS Account Important: if you useing EventEngine which provide AWS, you jump to Cloudformation template step.\n Already has AWS account, you can start now, but if you don`t have AWS account, first generate AWS account here.\n IAM User If you have created an AWS account but have not created an IAM user, you can create an IAM user using the IAM console. Follow the steps below to create an Administrator user. If you already have an admin user, skip the next IAM user creation task.\n Sign in to the IAM Console as the Root user in your AWS account using your AWS account email address and password. Select Users from the menu panel on the left side of the IAM console, then click Add user. Enter User name as Administrator. Select the AWS Management Console access checkbox, select Custom password, and enter your password. Click Next: Permissions.   Select Attach existing policies directly, select the checkbox for the AdministratorAccess policy, and click Next: Tags.   Click Next: Review. Verify that the AdministratorAccess managed policy has been added to the Administrator user, and click Create user. Now, logout the user logout and login as the newly created Administrator user. You can log in using the following URL:   https://\u0026lt;your_aws_account_id\u0026gt;.signin.aws.amazon.com/console/\n\u0026lt;your_aws_account_id\u0026gt; is the unique ID of your AWS account. As a root user, an error may occur when performing this exercise.\n  EC2 Key Pair To configure the basic environment required for the lab using the CloudFormaton template, you must provide an Amazon EC2 key pair. If you already have an EC2 key pair, skip the next task.\n Sign in to the AWS console as a Administrator user, and then go to the EC2 Console (https://console.aws.amazon.com/ec2/). In the navigation pane, select Key Pairs from Network \u0026amp; Security. Click Create Key Pair. Enter the name of the new key pair in Key pair name, then click Create. Private Key file in .PEM file format is automatically downloaded from the browser. The private key is required when using the following CloudFormation.  You must download .pem key. Do not use EventEngine\u0026rsquo;s default key.\n CloudFormation Template Create a CloudFormation stack using the CloudFormation-template provided to proactively create the AWS resources needed for the AWS AWS CodeQuality lab.\nCreate EC2 and VPC for 3-tier-web. We adopt best practices each pillar to this web.\nTo launch the CloudFormation stack, click the Launch Stack button to go to the CloudFormation console.\n Launch Stack  Typing MasterAccountStack in Stackname. Typing Prod in Workload Name.\nIn the stack creation step, enter a stack name and choose the EC2 key pair you created before. And in the final step, select Acknowledge checkbox and click Create stack so that CloudFormation can use a custom name when creating the IAM resource.\nCheck CloudFormation stack in Outputs tab, find ALBAddress, DBDNS.\n "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/setup/wellarchitectedtools/",
	"title": "Start AWS Well-Architected Review using Well-Architected tool",
	"tags": [],
	"description": "",
	"content": "Authors  Rodney Lester, Reliability Lead, Well-Architected, AWS +Seyong Kang, Solutions Architect  Introduction The purpose if this lab is to walk you through the features of the AWS Well-Architected Tool. You will create a workload, review the Reliability Pillar questions, save the workload, take a milestone, and examine and download the Well-Architected Review report.\nThe knowledge you acquire will help you build Well-Architected workloads in alignment with the AWS Well-Architected Framework\nGoals:  Learn where resources about the questions and best practices are located. Learn how to use milestones to track your progress again high and medium risks over time. Learn how to generate a report or view the results of the review in the Well-Architected Tool.  Steps:  Creating a workload   Performing a review   Saving a milestone   Viewing and downloading the report   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/performanceefficiency/",
	"title": "Performance Efficiency",
	"tags": [],
	"description": "",
	"content": "Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about Performance Efficiency on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper.\nLabs  Monitoring using the CloudWatch dashboard    View Amazon CloudWatch Automatic Dashboards   CloudWatch Custom Dashboard    CPU Alram using the CloudWatch    Custom Alram Configuration   Subscribe e-mail and mobile number   EC2 Loadtest    "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/performanceefficiency/cloudwatchdashboards/",
	"title": "Monitoring using the CloudWatch dashboard",
	"tags": [],
	"description": "",
	"content": "Authors  Eric Pullen, Performance Efficiency Lead Well-Architected +Seyong Kang, Solutions Architect  Introduction This hands-on lab will guide you through configuring an Amazon CloudWatch Dashboard to get aggregated views of the health and performance of all AWS resources. This enables you to quickly get started with monitoring, explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues. You can find more best practices by reading the Performance Efficiency Pillar of the AWS Well-Architected Framework. The skills you learn will help you secure your workloads in alignment with the AWS Well-Architected Framework.\nGoals  Monitor resources to ensure they are performing as expected  Let\u0026rsquo;s start!\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/performanceefficiency/cloudwatcheventemail/",
	"title": "CPU Alram using the CloudWatch",
	"tags": [],
	"description": "",
	"content": "Authors  +Seyong Kang, Solutions Architect  Introduce In this lab, you create a CPU usage alarm using the AWS Console. You can create a CloudWatch alarm that sends an email message using Amazon SNS. The alarm state changes to when the average CPU utilization of the EC2 instance exceeds the specified threshold in succession over the specified period.\nGoals make alram and start Autoscaling when CPU usage over 90%. Sending e-mail to user when CPU usage is over 90%.\nLet\u0026rsquo;s start\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/cost/",
	"title": "Cost Optimization",
	"tags": [],
	"description": "",
	"content": " Your browser doesn't support video, or if you're on GitHub head to https://wellarchitectedlabs.com to watch the video.  Introduction This repository contains documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about Cost Optimization on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Cost Optimization whitepaper.\nLabs  EC2 Right Sizing    Creating a workload   Performing a review   Saving a milestone   Viewing and downloading the report    EC2 Right Sizing(MEM)    Getting to know Amazon Cloudwatch   Attach CloudWatch IAM role to selected EC2 Instances   Cloudwatch Agent Manual Install   Updated Amazon EC2 Resource Optimization recommendations   Amazon EC2 Right Sizing Best Practices   Tear down    "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/",
	"title": "Operational Excellence",
	"tags": [],
	"description": "",
	"content": "Author  Mahanth Jayadeva, Solutions Architect, Well-Architected +Seyong Knag, Solutions Architect ++Jimin kim, Solutions Architect  For more information about Operational Excellence on AWS visit the Well-Architected tool in the AWS console, and read the AWS Well-Architected Operational Excellence whitepaper.\n What is Operational Excellence? The Operational Excellence pillar includes the ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures to deliver business value.\nDesign Principles for Operational Excellence Perform operations as code In the cloud, you can apply the same engineering discipline that you use for application code to your entire environment. You can define your entire workload (applications, infrastructure) as code and update it with code. You can implement your operations procedures as code and automate their execution by triggering them in response to events. By performing operations as code, you limit human error and enable consistent responses to events.\nMake frequent, small, reversible changes Design workloads to allow components to be updated regularly. Make changes in small increments that can be reversed if they fail (without affecting customers when possible).\nRefine operations procedures frequently As you use operations procedures, look for opportunities to improve them. As you evolve your workload, evolve your procedures appropriately. Set up regular game days to review and validate that all procedures are effective and that teams are familiar with them.\nAnticipate failure Perform “pre-mortem” exercises to identify potential sources of failure so that they can be removed or mitigated. Test your failure scenarios and validate your understanding of their impact. Test your response procedures to ensure that they are effective, and that teams are familiar with their execution. Set up regular game days to test workloads and team responses to simulated events.\nLearn from all operational failures Drive improvement through lessons learned from all operational events and failures. Share what is learned across teams and through the entire organization.\nSteps:  AWS Systems Manager   Resource Management   Patch Management   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/systemmanager/",
	"title": "AWS Systems Manager",
	"tags": [],
	"description": "",
	"content": " There are set up tasks and pre-requisites that must be satisfied prior to using Systems Manager to manage your EC2 instances or on-premises systems in hybrid environments.\n AWS Systems Manager AWS Systems Manager gives you visibility and control of your infrastructure on AWS. Systems Manager provides a unified user interface so you can view operational data from multiple AWS services and allows you to automate operational tasks across your AWS resources. With Systems Manager, you can group resources, like Amazon EC2 instances, Amazon S3 buckets, or Amazon RDS instances, by application, view operational data for monitoring and troubleshooting, and take action on your groups of resources. Systems Manager simplifies resource and application management, shortens the time to detect and resolve operational problems, and makes it easy to operate and manage your infrastructure securely at scale. Inventory AWS Systems Manager Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated. You can configure Inventory on all of your managed instances by using a one-click procedure. You can also configure and view inventory data from multiple AWS Regions and accounts.\nState Manager AWS Systems Manager State Manager is a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define.\nPatch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications.) You can use Patch Manager to install Service Packs on Windows instances and perform minor version upgrades on Linux instances. You can patch fleets of EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Amazon Linux, Amazon Linux 2, CentOS, Debian Server, Oracle Linux, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Ubuntu Server. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.\nMaintenance Windows AWS Systems Manager Maintenance Windows let you define a schedule for when to perform potentially disruptive actions on your instances such as patching an operating system, updating drivers, or installing software or patches. Maintenance Windows also lets you schedule actions on numerous other AWS resource types, such as Amazon Simple Storage Service (Amazon S3) buckets, Amazon Simple Queue Service (Amazon SQS) queues, AWS Key Management Service (AWS KMS) keys, and many more.\nRun Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.\nFor details, refer to Link.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/systemmanager/prepare/",
	"title": "Prepare",
	"tags": [],
	"description": "",
	"content": " you need to create a VPC endpoint for AWS Systems Manager, create an IAM role, and assign it to EC2.\n 1. Generate VPC Endpoint In the lab environment, EC2 is located in the private subnet. In this lab, we will configure AWS Systems Manager to connect privately through a VPC endpoint without public IP.\n Go to the AWS console, Search VPC or click this link:VPC. And click endpoint on left top. Click Create endpoint button.  in Service category, click AWS Service and search EC2 in Service Name search bar. Select com.amazonaws.\u0026lt;region\u0026gt;.ec2 and scroll down, click Create endpoint button.  Select VPC. Search and click WellArchitectedLabsStack/VPC which generate lab setup. Subnets select checkbox ap-northeast-2b and ap-northeast-2b, check all PrivateSubnet each AZ.  Select Security Group. Select Description value is WellArchitectedLabsStack/ASG/InstanceSecurityGroup which for EC2 instance.  Same way search SSM, Select com.amazonaws.\u0026lt; region \u0026gt;.ssm. The final created endpoint should have at least 2 endpoints, ec2 and ssm, as shown below.  com.amazonaws.ap-northeast-2.ec2 com.amazonaws.ap-northeast-2.ssm     2. IAM 역할 생성 및 EC2에 할당 1) Managed Instance 확인  Open to the Systems Manager Console. For reference, the EC2 instance for the practice is Amazon Linux AMI 2017.09, and since SSM agent is installed by default, installing the agent on the instance is omitted. Left menu, Click Managed Instances.  Before you create an IAM role and assign it to EC2, you can see that the instances managed by Systems Manager are not yet looked up.     2) Create an IAM role: AWS Systems Manager creates a profile to be used by instances to be managed   Go to IAM console.\n  In left menu, click Roles.\n  Click Create role.\n  Section in Select type of trusted entity, Click AWS service.\n  And Section Choose the service that will use this role, Click EC2(EC2 Allows EC2 instances to call AWS services on your behalf). And Click Next Permission Button.   under Attached permissions policy , search AmazonEC2RoleforSSM and select. and click Next: Tags.   Click Next Review button.\n  Review Selection:\n type Role name: ManagedInstancesRole. if you need type Role description. Click Create role.    3) Attach IAM to EC2 Creating an IAM role to attach to the EC2 instance that you want to manage with AWS Systems Manager. Attach the ManagedInstancesRole you just created to the instance you want to manage with Systems Manager. Target is 2 EC2 instance whitch name WellArchitectedLabsStack/ASG, one Bastion Host.\n Go to EC2 Console, Click Instances. Click instance to attach IAM role, Click top menu button Actions, Instance Settings, and Attach/Replace IAM Role.  Search ManagedInstancesRole of Attach/Replace IAM Role, select role and click Apply.  Attach ManagedInstancesRole role to other instance which managed from Systems Manager.  4) Managed Instance reconfirm   Go to Systems Manager console, click Managed Instances in menu. Select Managed Instances periodically until the instance appears in the list. It may take a few minutes for the instance to appear in the Managed Instances list.   Finally, you can see the instances registered with the Managed Instance.   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/inventory/",
	"title": "Resource Management",
	"tags": [],
	"description": "",
	"content": " AWS Systems Manager는이 실습 전체에서 살펴볼 IT 운영을 지원하는 기능 모음입니다. AWS Systems Manager 인벤토리를 사용하여 하이브리드 환경의 Amazon EC2 인스턴스와 온 프레미스 서버 또는 가상 머신 (VM)에서 운영 체제 (OS), 애플리케이션 및 인스턴스 메타 데이터를 수집 할 수 있습니다. 메타 데이터를 쿼리하여 소프트웨어 정책에 필요한 소프트웨어 및 구성을 실행중인 인스턴스와 업데이트해야하는 인스턴스를 빠르게 이해할 수 있습니다.\n Systems Manager Prereqs There are setup tasks and prerequisites that must be met before using Systems Manager to manage EC2 instances or on-premises systems in a hybrid environment.\n You must use a supported operating system  Supported operating systems include versions of Windows, Amazon Linux, Ubuntu Server, RHEL, and CentOS   The SSM Agent must be installed  The SSM Agent for Windows also requires PowerShell 3.0 or later to run some SSM documents   Your EC2 instances must have outbound internet access You must access Systems Manager in a supported region Systems Manager requires IAM roles  for instances that will process commands for users executing commands    SSM Agent is installed by default on:\n Amazon Linux base AMIs dated 2017.09 and later Windows Server 2016 instances Instances created from Windows Server 2003-2012 R2 AMIs published in November 2016 or later  Inventory AWS Systems Manager Inventory provides visibility into your Amazon EC2 and on-premises computing environment. You can use Inventory to collect metadata from your managed instances. You can store this metadata in a central Amazon Simple Storage Service (Amazon S3) bucket, and then use built-in tools to query the data and quickly determine which instances are running the software and configurations required by your software policy, and which instances need to be updated. You can configure Inventory on all of your managed instances by using a one-click procedure. You can also configure and view inventory data from multiple AWS Regions and accounts.\nState Manager AWS Systems Manager State Manager is a secure and scalable configuration management service that automates the process of keeping your Amazon EC2 and hybrid infrastructure in a state that you define.\nSteps:  Inventory   State Manager   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/patch/",
	"title": "Patch Management",
	"tags": [],
	"description": "",
	"content": "In the cloud, you can apply the same engineering principles you use for application code across your entire environment. The entire workload (application, infrastructure, etc.) can be defined in code and updated with code. You can script and automate execution of task procedures by triggering task procedures in response to events. Performing work in code reduces human error and helps ensure consistent execution of work activities.\nSystems Manager features related to this lab.\nPatch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with both security related and other types of updates. You can use Patch Manager to apply patches for both operating systems and applications. (On Windows Server, application support is limited to updates for Microsoft applications.) You can use Patch Manager to install Service Packs on Windows instances and perform minor version upgrades on Linux instances. You can patch fleets of EC2 instances or your on-premises servers and virtual machines (VMs) by operating system type. This includes supported versions of Windows Server, Amazon Linux, Amazon Linux 2, CentOS, Debian Server, Oracle Linux, Red Hat Enterprise Linux (RHEL), SUSE Linux Enterprise Server (SLES), and Ubuntu Server. You can scan instances to see only a report of missing patches, or you can scan and automatically install all missing patches.\nRun Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. A managed instance is any EC2 instance or on-premises machine in your hybrid environment that has been configured for Systems Manager. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs. Run Command is offered at no additional cost.\nIn this lab, you apply the concepts of Infrastructure as Code and Operations of Code to the following activities.\nSteps:  Patch Manager   Run Command   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/reliability/",
	"title": "Reliability",
	"tags": [],
	"description": "",
	"content": "These hands-on labs will teach you how to implement reliable workloads using AWS.\n Reliability is the ability of a workload to perform its intended function correctly and consistently when it’s expected to. This includes the ability to operate and test the workload through its total lifecycle Resiliency is the ability of a workload to recover from infrastructure or service disruptions, dynamically acquire computing resources to meet demand, and mitigate disruptions, such as misconfigurations or transient network issues. Reliability depends on multiple factors, of which resiliency is one the most impactful.  For more information about Reliability, read the AWS Well-Architected Reliability whitepaper.\nReliability Labs  Testing for Resiliency of EC2, RDS, and AZ    CloudFront with WAF Protection    "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "Introduction The security labs are documentation and code in the format of hands-on labs to help you learn, measure, and build using architectural best practices.\nFor more information about security on AWS visit AWS Security and read the AWS Well-Architected Security whitepaper in PDF or online. Also check out https://awssecworkshops.com/ for hands-on workshops, and AWS Training and Certification Learning Library for official security training options.\nLabs \u0026amp; Quests  CloudFront with WAF Protection    AWS WAF Config   CloudFront Config - EC2 or Load Balancer    "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/review/",
	"title": "Review Again(Machanism)",
	"tags": [],
	"description": "",
	"content": "So far, we\u0026rsquo;ve applied the best practices provided by 5 pillars on the 3-tier web. Now, let\u0026rsquo;s look at the things that have changed from the first review.\nFirst Environment Best Practices Applied If you have performed HoL for each pillar, you can apply the following best practices to your existing environment.\n   pillar name description     Performance Efficiency Monitoring with CloudWatch Use CloudWatch\u0026rsquo;s dashboard to measure and monitor your current EC2 usage.   Performance Efficiency Alarm with CloudWatch When the CPU usage monitored by CloudWatch rises above a certain level, an alarm is sent to the user via e-mail.   Cost Optimization Right Sizeing of EC2 Activate Cost Explorer\u0026rsquo;s Recommendation to find the appropriate size of the EC2 you are using. Recommend and change the appropriate EC2 for your usage.   Cost Optimization Right Sizing accuracy improvement through memory metric collection of CloudWatch Agent You can improve the accuracy of right sizing by collecting additional memory metrics.   Operational Excellence Resource Central Management with System Manager Collects, monitors, and manages information on EC2 instances created through System Mangager\u0026rsquo;s inventory.   Operational Excellence Patch management using System Manager Easily apply and automate security-related and updates to instances through System Manager\u0026rsquo;s patch manager.   Reliability Workload reliability testing with Chaos testing Through a script that stops EC2, RDS, and AZ, it verifies the resilience of the workload in the event of a corresponding failure and evaluates how the system responds to various failure scenarios.    Environment after the first Well-Architected Framework Review Now, in line with the best practices we\u0026rsquo;ve applied, we\u0026rsquo;ll go back to the workload review of Well-Architected tools.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/performanceefficiency/cloudwatchdashboards/default-dashboard/",
	"title": "View Amazon CloudWatch Automatic Dashboards",
	"tags": [],
	"description": "",
	"content": "1. View Amazon CloudWatch Automatic Dashboards automatic_dashboards Amazon CloudWatch Automatic Dashboards allow you to easily monitor all AWS Resources, and is quick to get started. Explore account and resource-based view of metrics and alarms, and easily drill-down to understand the root cause of performance issues.\n  Open the Amazon CloudWatch console at https://console.aws.amazon.com/cloudwatch/ and select your region from the top menu bar.\n  If you are logging into a brand new AWS account, you will see the default Cloudwatch console such as this: You will need to deploy something into your account to see a Cloudwatch automatic dashboard.\n  Once you have services deployed into your AWS account, Cloudwatch will automatically populate the Overview tab with various metrics such as this:   The upper left shows a list of AWS services you use in your account, along with the state of alarms in those services. In this example, it is showing that we have an EC2 instance in use and it is marked as OK.   The upper right shows alarms in your account, which will contain up to four alarms that are in the ALARM state or it will show those that most recently changed state. These upper areas enable you to assess the health of your AWS services, by seeing the alarm states in every service and the alarms that most recently changed state. This helps you monitor and quickly diagnose issues.\n  Below these areas is spot for a custom default dashboard that you can create that is named CloudWatch-Default This is a convenient way for you to add metrics about your own custom services or applications to the overview page, or to bring forward additional key metrics from AWS services that you most want to monitor. In this example, we do not have a custom default dashboard created.  If you wish, you can click the \u0026ldquo;Create a new CloudWatch-Default dashboard\u0026rdquo; to generate a new dashboard and see it displayed in the overview screen.    If you use six or more AWS services, below the default dashboard is a link to the automatic cross-service dashboard. The cross-service dashboard automatically displays key metrics from every AWS service you use without requiring you to choose what metrics to monitor or create custom dashboards. You can also use it to drill down to any AWS service and see even more key metrics for that service. In this example, we see both EC2 metrics as well as EBS volume metrics for the test machines that were created.\n  Let\u0026rsquo;s make custom cloudwatch dashboard.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/performanceefficiency/cloudwatchdashboards/setup/",
	"title": "CloudWatch Custom Dashboard",
	"tags": [],
	"description": "",
	"content": "CloudWatch Custom Dashboard  Let\u0026rsquo;s measure CPU utilization and Instance Count attached ALB.\n  First you need Auto scaling group name, and Application load balancer name.\nGo to CloudFormation console : https://ap-northeast-2.console.aws.amazon.com/cloudformation/home?region=ap-northeast-2\n  Output\u0026rsquo;s ASGName is name of Auto scaling group. ALBName is name of Application Load Balancer Click MasterAccountStack at CloudFormation console. Click Output tab and copy somewhere value ASGNameand ALBName. Let`s make CloudWatch Dashboard. Go to CloudWatch : https://ap-northeast-2.console.aws.amazon.com/cloudwatch/home?region=ap-northeast-2\n  Select Dashboards where right top.   Click Create Dashboard button.   typing at warworkshop-default dashboard name. And click Create dashboard button for generate basic dashboard.   Now we add widget for watch EC2 CPU utilization at dashboard. Select Line Compare metrics over time and click Configure.   Paste the valut of ASGName from the previously copied output into the All metrics search bar. And click enter and choose EC2 \u0026gt; By Auto Scaling Group   You can show variable Metrics. Now click CPUUtilization Metric name. Click Create Widget button.   Check dashboard added CPUUtilization. next we add EC2 instance count widget. Click Add Widget button at menu bar   In this case, We use Number type. Select Number type and click Configure button.   Paste the valut of ASGName from the previously copied output into the All metrics search bar. And click enter and choose ApplicationELB \u0026gt; Per AppELB, per TG Metrics   Select HealthyHostCount and click Create widget button.   You can see that there are very low CPU usage and 2 instances. Save the dashboard.   Now let\u0026rsquo;s create a Cloudwatch alarm so users can immediately notice changes in CPU utilization.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/performanceefficiency/cloudwatcheventemail/setup/",
	"title": "Custom Alram Configuration",
	"tags": [],
	"description": "",
	"content": "CloudWatch Alram  We will measure the CPU usage and number of instances in WellArchitectedFrameworkLabsStack which created in CloudFormation.\nOpen CloudWatch : https://console.aws.amazon.com/cloudwatch/\n  Left menu, select Alarms, and click Create alarm   Click Select Metric.   Paste the valut of ASGName from the previously copied output into the All metrics search bar. Select EC2 \u0026gt; By Auto Scaling group.   Find the Metric CPUUtilization, and check the box, and click the Select Metric button.   Let`s configuration Specify metric and conditions. We make alram when CPUUtilization metric over 90%. Select p90 in Statistic. Period select 1 minute.   Conditions `s threshold type select Static. Whenever CPUUtilization is set Greater. and type in than.. 95.   Click Next.\n  Let\u0026rsquo;s make new SNS topic. Select in alarm at Alarm state trigger, check Create new topic radio bax in Select an SNS topic panal. and typing Default_CloudWatch_Alarms_Topic in Create a new topic\u0026hellip; box, enter your email address in Email Endpoint, click Create topic button.   Click Next.\n  Enter the following in Add Alarm name and Alarm description.\n Alarm name : CloudWatch-CPU-Alarm Alarm description - optional : for WAR workshop     Click Create alram.\n  let\u0026rsquo;s add my email and mobile to alram.\nor you can start load test right now!\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/performanceefficiency/cloudwatcheventemail/snstopic/",
	"title": "Subscribe e-mail and mobile number",
	"tags": [],
	"description": "",
	"content": "SNS e-mail Confirm  email attached SNS topic have to re-approved in the account to receive Alarm. Find the email from ÀWS Notification in the your inbox when setting up the previous step. Click Confirm subription. finished subscribe. The Step below are available in regions that provide SNS.(서울리전은 불가)\n Now, let\u0026rsquo;s add a mobile phone number to the SNS topic so that we can send a mobile text message when an alarm occurs.\nOpen Amazon SNS console : https://console.aws.amazon.com/sns/\n  Select Topics from the left menu.   Select the SNS topic Default_CloudWatchAlarms_Topic that you created while creating the alarm.   Scroll down and look at Subscriptions to see the email you registered when you created the alert. You can add new subscribers here by selecting Create Subscription.   Select Protocol of subscription creation by SMS. (You can use various protocols here.) Then, enter your mobile phone number in the endpoint. (ex+821012345678) And click the Create Subscription button at the bottom.   If you select Default_CloudWatchAlarms_Topic from the previous Topics menu, you can see the email and mobile phone information associated with the topic.\n  Now let\u0026rsquo;s see what happens when the CPU usage of EC2 Instance increases.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/performanceefficiency/cloudwatcheventemail/ec2/",
	"title": "EC2 Loadtest",
	"tags": [],
	"description": "",
	"content": "After increasing the CPU usage of EC2, we will check the dashboard:  When the CPU usage increases, Autoscaling Group automatically increases the number of EC2 Instances. And we\u0026rsquo;ll see that change on Cloudwatch\u0026rsquo;s dashboard.\nWeb access through application load balancer   The DNS address of the Application Load balancer is in the Output in WellArchitectedFrameworkLabsStack created before.\nOpen theCloudFormation Console : https://ap-northeast-2.console.aws.amazon.com/cloudformation/home?region=ap-northeast-2\n  Select WellArchitectedFrameworkLabsStack.   The ALBDNS in output is the DNS address of the Application Load Balancer. After selecting the output in the upper tab, copy the value of ALBDNS and put it in the address bar of your web browser.   You can see the this screen. The CPU usage of this web server is 0%. When you click the Load test button at the top, the screen changes and the CPU load of EC2 rises to 100%. CPU load will not go down until you exit this screen.\n  Let\u0026rsquo;s go to the CloudWatch console and see the user dashboard we created earlier.\nOpen the CloudWatch : https://console.aws.amazon.com/cloudwatch/\n  Select Dashboards on right top.   Select the warworkshop-default.   The monitoring default pefiod is 5 minutes (this can be changed in settings).Refresh after 5 minutes and you will see the CPUUtilization graph that has changed drastically towards 100%. And after a little more time, you can see that the Autoscaling group has expanded up to 4 instances.   Also check your mail. You can check that the alert has been sent to the previously registered email. If you have registered your mobile phone number, you can also receive text messages.\n  If you go to the EC2 Dashboard, you can also check the automatically expanded EC2.   you leave the loadtest window, after a while you can see the two instances and the stable graph.\n  Use Cloudwatch to monitor optimal performance. System performance can degrade over time. Monitor system performance to identify degraded conditions and resolve internal or external factors such as operating system or application load. Create an alarm to check changes in system performance.\n "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/inventory/inventorysetup/",
	"title": "Inventory",
	"tags": [],
	"description": "",
	"content": "Systems Manager: Inventory You can use AWS Systems Manager Inventory to collect operating system (OS), application, and instance metadata from your Amazon EC2 instances and your on-premises servers or virtual machines (VMs) in your hybrid environment. You can query the metadata to quickly understand which instances are running the software and configurations required by your software policy, and which instances need to be updated.\nUsing Systems Manager Inventory to Track Your Instances  Under Instances \u0026amp; Nodes in the AWS Systems Manager navigation bar, choose Inventory.  Scroll down in the window to the Corresponding managed instances section. Inventory currently contains only the instance data available from the EC2  Choose the InstanceID of one of your systems.  Examine each of the available tabs of data under the Instance ID heading.The Inventory tab will be empty as it is not yet set up for inventory.   Inventory collection must be specifically configured and the data types to be collected must be specified. Limiting inventory collection targets based on resource tags You can also select all managed instances in your account to get your inventory, but you can also choose inventory instances based on specific tags, such as Environment or Workload, depending on your needs. Of course, you can also manually select specific instances.\n  Choose Inventory in the navigation bar. Choose Setup Inventory in the top right corner of the window  In the Setup Inventory screen, define targets for inventory:  Under Specify targets by, select Specifying a tag Under Tags specify Environment for the key and MasterAccount for the value     Schedule the frequency with which inventory is collected\n type InventoryAllInstances in Name of inventory. For Collect inventory data every, accept the default 30 Minute(s)  Under parameters, specify what information to collect with the inventory process  Please refer to the link to learn more about each parameter.     Not check Sync inventory execution logs to an S3 bucket.  reviewing the settings, click Setup Inventory at the bottom right. It can take up to 10 minutes to deploy the new inventory policy to your instances.  Several Inventory specifications can also be created. These are stored as Associations within the Systems Manager State Manager. We will learn more in the next chapter.\n   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/inventory/statemanager/",
	"title": "State Manager",
	"tags": [],
	"description": "",
	"content": "Systems Manager: State Manager Association In State Manager, an association is the result of binding configuration information that defines the state you want your instances to be in to the instances themselves. This information specifies when and how you want instance-related operations to run that ensure your Amazon EC2 and hybrid infrastructure is in an intended or consistent state.\nIn fact, we already made a link in the previous chapter, Inventory.   Navigate to Managed Instances under Instances and Nodes in the navigation bar. An Association Status has been established for the inventoried instances under management.   Choose one of the Instance ID links to go to the inventory of the instance.   The Inventory tab is now populated and you can track associations and their last activity under the Associations tab.   Navigate to Compliance under Instances \u0026amp; Nodes in the navigation bar. Here you can view the overall compliance status of your managed instances in the Compliance Summary and the individual compliance status of systems in the Corresponding managed instances section below.   Systems Manager: Compliance You can use AWS Systems Manager Configuration Compliance to scan your fleet of managed instances for patch compliance and configuration inconsistencies. You can collect and aggregate data from multiple AWS accounts and Regions, and then drill down into specific resources that aren’t compliant.\nBy default, Configuration Compliance displays compliance data about Systems Manager Patch Manager patching and Systems Manager State Manager associations. You can also customize the service and create your own compliance types based on your IT or business requirements. You can also port data to Amazon Athena and Amazon QuickSight to generate fleet-wide reports.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/patch/patchmanager/",
	"title": "Patch Manager",
	"tags": [],
	"description": "",
	"content": "Systems Manager: Patch Manager AWS Systems Manager Patch Manager automates the process of patching managed instances with security related updates.\n Warnings\n The operating systems supported by Patch Manager may vary from those supported by the SSM Agent. AWS does not test patches for Windows or Linux before making them available in Patch Manager . If any updates are installed by Patch Manager the patched instance is rebooted. Always test patches thoroughly before deploying to production environments.   Patch Baselines Patch Manager uses patch baselines, which include rules for auto-approving patches within days of their release, as well as a list of approved and rejected patches. Later in this lab we will schedule patching to occur on a regular basis using a Systems Manager Maintenance Window task. Patch Manager integrates with AWS Identity and Access Management (IAM), AWS CloudTrail, and Amazon CloudWatch Events to provide a secure patching experience that includes event notifications and the ability to audit usage.\n5.1 Create a Patch Baseline   Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Patch Manager.\n  Click the View predefined patch baselines link under the Configure patching button on the upper right.   Choose Create patch baseline.   On the Create patch baseline page in the Provide patch baseline details section:\n Enter a Name for your custom patch baseline, such as AmazonLinuxSecAndNonSecBaseline. Optionally enter a description, such as Amazon Linux patch baseline including security and non-security patches. Select Amazon Linux2 to Operation system.     In the Approval rules section:\n Examine the options in the lists and ensure that Product, Classification, and Severity have values of All. Leave the Auto approval delay at its default of 0 days. Change the value of Compliance reporting - optional to Critical.  Choose Add another rule. In the new rule, change the value of Compliance reporting - optional to Medium. Check the box under Include non-security updates to include all Amazon Linux updates when patching.     In the Patch exceptions section in the Rejected patches - optional text box, enter system-release.* This will reject patches to new Amazon Linux releases that may advance you beyond the Patch Manager supported operating systems prior to your testing new releases.\n  For Linux operating systems, you can optionally define an alternative patch source repository. Choose the X in the Patch sources area to remove the empty patch source definition. Choose Create patch baseline and you will go to the Patch Baselines page where the AWS provided default patch baselines, and your custom baseline, are displayed.   Click Create patch baseline button and finish. If an approved patch is reported as missing, the option you choose in Compliance reporting, such as Critical or Medium, determines the severity of the compliance violation reported in System Manager Compliance.\n  Patch Groups A patch group is an optional method to organize instances for patching. For example, you can create patch groups for different operating systems (Linux or Windows), different environments (Development, Test, and Production), or different server functions (web servers, file servers, databases). Patch groups can help you avoid deploying patches to the wrong set of instances. They can also help you avoid deploying patches before they have been adequately tested.\nYou create a patch group by using Amazon EC2 tags. Unlike other tagging scenarios across Systems Manager, a patch group must be defined with the tag key: Patch Group (tag keys are case sensitive). You can specify any value (for example, web servers) but the key must be Patch Group.\n NoteAn instance can only be in one patch group.\n After you create a patch group and tag instances, you can register the patch group with a patch baseline. By registering the patch group with a patch baseline, you ensure that the correct patches are installed during the patching execution. When the system applies a patch baseline to an instance, the service checks if a patch group is defined for the instance.\n If the instance is assigned to a patch group, the system checks to see which patch baseline is registered to that group. If a patch baseline is found for that group, the system applies that patch baseline. If an instance isn\u0026rsquo;t assigned to a patch group, the system automatically uses the currently configured default patch baseline.  Assign a Patch Group  Choose the Baseline ID of your newly created baseline to enter the details screen.(If you don\u0026rsquo;t see the Baseline ID, turn the page. The baseline name created in this lab is AmazonLinuxSecAndNonSecBaseline.)  Choose Actions in the top right of the window and select Modify patch groups.  In the Modify patch groups window under Patch groups, enter Critical, choose Add, and then choose Close to be returned to the Patch Baseline details screen.  Click Close, and back to Patch Baseline page for check detail information.    Note\n If an instance is assigned to a patch group, the system checks based on the patch registered in that group. If there is a patch criterion for that group, the system applies that patch criterion. Except for Bastion Host, the two instances belong to a patch group called Critical, so they follow the patch criteria you just created. *If an instance is not assigned to a patch group, the system will automatically use the default patch baseline currently configured.   "
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/operational-excellence/patch/awsrunpatchbaseline/",
	"title": "Run Command",
	"tags": [],
	"description": "",
	"content": "AWS Systems Manager: Run Command AWS Systems Manager Run Command lets you remotely and securely manage the configuration of your managed instances. Run Command enables you to automate common administrative tasks and perform ad hoc configuration changes at scale. You can use Run Command from the AWS Management Console, the AWS Command Line Interface, AWS Tools for Windows PowerShell, or the AWS SDKs.\nIn this lab, we use Run Command to patching.\nAWS Systems Manager: Documents An AWS Systems Manager document defines the actions that Systems Manager performs on your managed instances. Systems Manager includes many pre-configured documents that you can use by specifying parameters at runtime, including \u0026lsquo;AWS-RunPatchBaseline\u0026rsquo;. These documents use JavaScript Object Notation (JSON) or YAML, and they include steps and parameters that you specify.\nAll AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents. You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities.\nAWS-RunPatchBaseline All AWS provided Automation and Run Command documents can be viewed in AWS Systems Manager Documents. You can create your own documents or launch existing scripts using provided documents to implement custom operations as code activities.\n1. Examine AWS-RunPatchBaseline in Documents  Click in the search box, select Document name prefix, and then Equal.  Click search box, Document name prefixand Click Equal. Type AWS-Run into the text field and press Enter on your keyboard to start the search.  Select AWS-RunPatchBaseline and choose View details.  Review the content of each tab in the details page of the document.   Scan Your Instances with AWS-RunPatchBaseline via Run Command  Under Instances and Nodes in the AWS Systems Manager navigation bar, choose Run Command. In the Run Command dashboard, you will see previously executed commands including the execution of AWS-RefreshAssociation, which was performed when you set up inventory. Choose Run Command in the top right of the window.  In the Run a command window, under Command document:  Choose the search icon and select Platform types, and then choose Linux to display all the available commands that can be applied to Linux instances. Choose AWS-RunPatchBaseline in the list.    In the Command parameters section, leave the Operation value as the default Scan.  In the Targets section:  Select Specify instance tags. Under Enter a tag key, enter Workload, and under Enter a tag value, enter Prod and click Add.      Note The remaining Run Command features enable you to:\n  Specify Rate control, limiting Concurrency to a specific number of targets or a calculated percentage of systems, or to specify an Error threshold by count or percentage of systems after which the command execution will end. Specify Output options to record the entire output to a preconfigured S3 bucket and optional S3 key prefix.   NoteOnly the last 2500 characters of a command document\u0026rsquo;s output are displayed in the console.\n  Specify SNS notifications to a specified SNS Topic on all events or on a specific event type for either the entire command or on a per-instance basis. This requires Amazon SNS to be preconfigured. View the command as it would appear if executed within the AWS Command Line Interface.   Choose Run to execute the command and return to its details page. Scroll down to Targets and outputs to view the status of the individual targets that were selected through your tag key and value pair. Refresh your page to update the status. Choose an Instance ID from the targets list to view the Output from command execution on that instance. Choose Step 1 - Output to view the first 2500 characters of the command output from Step 1 of the command, and choose Step 1 - Output again to conceal it.  Choose Step 2 - Output to view the first 2500 characters of the command output from Step 2 of the command. The execution step for PatchWindows was skipped as it did not apply to your Amazon Linux instance.  Review Patch Compliance After Patching  Under Instances \u0026amp; Nodes in the the AWS Systems Manager navigation bar, choose Compliance. The Compliance resources summary will now show that there are 4 systems that have satisfied critical severity patch compliance.  The Impact of Operations as Code In a traditional environment, you would have had to set up the systems and software to perform these activities. You would require a server to execute your scripts. You would need to manage authentication credentials across all of your systems.\nOperations as code reduces the resources, time, risk, and complexity of performing operations tasks and ensures consistent execution. You can take operations as code and automate operations activities by using scheduling and event triggers. Through integration at the infrastructure level you avoid \u0026ldquo;swivel chair\u0026rdquo; processes that require multiple interfaces and systems to complete a single operations activity.\n"
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/tags/test_resiliency/",
	"title": "test_resiliency",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://wellarchitected.workshop.kr.s3-website-us-west-2.amazonaws.com/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
}]